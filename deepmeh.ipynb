{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd06c5a2c9b1f07f733e7d9da253458af018695fcb515ff3a6d52f2d57765146efa",
   "display_name": "Python 3.8.3 64-bit ('tired': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "4b3fe4e430614ae3e36b8912aa67966595d32d796c9f5e46d3c96e8e35c078a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import numpy.fft as fft\n",
    "# import mne\n",
    "from numpy import save, load\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.utils import np_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "num gpus available [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\nTrue\n"
     ]
    }
   ],
   "source": [
    "gpus=tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"num gpus available\", gpus)\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "source": [
    "### read dataset X and Y"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "84\n",
      "(14584, 208)\n",
      "72\n",
      "(27194, 208)\n",
      "76\n",
      "(40766, 208)\n",
      "24\n",
      "(55977, 208)\n",
      "78\n",
      "(70200, 208)\n",
      "36\n",
      "(94130, 208)\n",
      "38\n",
      "(118212, 208)\n",
      "40\n",
      "(125347, 208)\n",
      "38\n",
      "(149718, 208)\n",
      "50\n",
      "(167627, 208)\n"
     ]
    }
   ],
   "source": [
    "subject_id=1\n",
    "base_path = \"features/\"\n",
    "# edf_file_names = sorted(glob.glob(os.path.join(base_path, \"data_chb01/*.npy\".format(subject_id))))\n",
    "# files=len(edf_file_names)\n",
    "X=load('features/data_chb01/features_{}_00.npy'.format(subject_id))\n",
    "y=load('features/data_chb01/targets_{}_00.npy'.format(subject_id))\n",
    "for subject_id in range(1,11):\n",
    "    edf_file_names = sorted(glob.glob(os.path.join(base_path, \"data_chb{:02d}/*.npy\".format(subject_id))))\n",
    "    files=len(edf_file_names)\n",
    "    print(files)\n",
    "    \n",
    "    start=0\n",
    "    if subject_id==1:\n",
    "        start=1\n",
    "    for fileno in range(start, files//2):\n",
    "        X=np.concatenate((X, load('features/data_chb{:02d}/features_{}_{:02d}.npy'.format(subject_id, subject_id, fileno))))\n",
    "        y=np.concatenate((y, load('features/data_chb{:02d}/targets_{}_{:02d}.npy'.format(subject_id,subject_id, fileno))))\n",
    "    print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(167627, 208) (167627,)\n"
     ]
    }
   ],
   "source": [
    "X_shape, y_shape = X.shape, y.shape\n",
    "print(X_shape, y_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(167627, 209)\n",
      "(167626, 209)\n"
     ]
    }
   ],
   "source": [
    "df=pd.DataFrame(data=X)\n",
    "df['target']=y\n",
    "print(df.shape)\n",
    "df=df.dropna()\n",
    "print(df.shape)"
   ]
  },
  {
   "source": [
    "### train test split"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(144326, 209) (8075, 209)\n"
     ]
    }
   ],
   "source": [
    "df_interictal=df[df['target']==0]\n",
    "# print(df_interictal.shape)\n",
    "# df_interictal=df_interictal.sample(frac=0.7)\n",
    "df_preictal=df[df['target']==2]\n",
    "df_preictal = df_preictal.replace({'target': 2}, 1)\n",
    "interictal_shape, preictal_shape = df_interictal.shape, df_preictal.shape\n",
    "print(df_interictal.shape, df_preictal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_interictal=np.array(df_interictal[df.columns[:-1]]).astype('float32')\n",
    "y_interictal=np.array(df_interictal['target']).astype('float32')\n",
    "X_preictal  =np.array(df_preictal[df.columns[:-1]]).astype('float32')\n",
    "y_preictal  =np.array(df_preictal['target']).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(144326, 208)\n(144326,)\n(8075, 208)\n(8075,)\n"
     ]
    }
   ],
   "source": [
    "print(X_interictal.shape)\n",
    "print(y_interictal.shape)\n",
    "print(X_preictal.shape)\n",
    "print(y_preictal.shape)"
   ]
  },
  {
   "source": [
    "### train test split"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_interictal_train, X_interictal_test, y_interictal_train, y_interictal_test =train_test_split(X_interictal,y_interictal,test_size=0.1, random_state=42)\n",
    "X_preictal_train, X_preictal_test, y_preictal_train, y_preictal_test=train_test_split(X_preictal, y_preictal,test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_interictal_train, X_preictal_train))\n",
    "X_test = np.concatenate((X_interictal_test, X_preictal_test))\n",
    "y_train = np.concatenate((y_interictal_train, y_preictal_train))\n",
    "y_test = np.concatenate((y_interictal_test, y_preictal_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(137160, 208)\n(15241, 208)\n(137160,)\n(15241,)\n"
     ]
    }
   ],
   "source": [
    "X_train_shape =X_train.shape\n",
    "X_test_shape = X_test.shape\n",
    "y_train_shape =y_train.shape \n",
    "y_test_shape = y_test.shape\n",
    "print(X_train_shape)\n",
    "print(X_test_shape)\n",
    "print(y_train_shape)\n",
    "print(y_test_shape)"
   ]
  },
  {
   "source": [
    "### Normalization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "X_train=scaler.transform(X_train)\n",
    "X_test=scaler.transform(X_test)"
   ]
  },
  {
   "source": [
    "### encoding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(15241, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "# encoded_Y = encoder.fit_transform(y)\n",
    "enc_y_train = np_utils.to_categorical(y_train)\n",
    "enc_y_test = np_utils.to_categorical(y_test)\n",
    "print(enc_y_test.shape)"
   ]
  },
  {
   "source": [
    "### training the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        Dense(256, name=\"layer1\", input_shape=(208,), activation = 'linear' ),\n",
    "        Dense(256, name=\"hidden1\", activation = 'linear'),\n",
    "        Dense(128, name=\"hidden2\", activation = 'linear'),\n",
    "        Dense(128, name=\"hidden3\", activation = 'linear'),\n",
    "        Dense(64, name=\"hidden4\", activation = 'linear'),\n",
    "        Dense(32, name=\"hidden5\", activation = 'linear'),\n",
    "        Dense(2, name=\"output\", activation = 'sigmoid'),\n",
    "    ]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-10dd813254c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy', tf.keras.metrics.FalseNegatives(thresholds=None, name=None, dtype=None)\n\u001b[0m\u001b[0;32m      2\u001b[0m ])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy', tf.keras.metrics.FalseNegatives(thresholds=None, name=None, dtype=None)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "01:41:36\n",
      "7694/7694 [==============================] - 16s 2ms/step - loss: 0.0962 - accuracy: 0.7580 - false_negatives_1: 28844.3531 - val_loss: 0.4838 - val_accuracy: 0.7570 - val_false_negatives_1: 6400.0000\n",
      "01:41:52\n",
      "saving new model\n",
      "INFO:tensorflow:Assets written to: DNN7.pkl\\assets\n"
     ]
    }
   ],
   "source": [
    "before = datetime.now()\n",
    "before_time =before.strftime(\"%H:%M:%S\")\n",
    "print(before_time)\n",
    "class_weight = {0: 0.1, 1: 0.9}\n",
    "model.fit(X_train, enc_y_train, epochs=1, batch_size= 32, validation_data=(X_test, enc_y_test), class_weight=class_weight)\n",
    "\n",
    "after = datetime.now()\n",
    "after_time =after.strftime(\"%H:%M:%S\")\n",
    "print(after_time)\n",
    "\n",
    "print (\"saving new model\")\n",
    "model.save(\"DNN7.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_clf = joblib.load(\"my_models/SVM_blanced_chb04.pkl\")\n",
    "# y_pred=loaded_clf.predict(X_test)\n",
    "y_pred1=model.predict(X_test)\n",
    "y+pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=[]\n",
    "for pr in y_pred1:\n",
    "    y_pred=np.argmax(pr)\n",
    "y_pred=np.array(y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'break_ties': False,\n",
       " 'cache_size': 200,\n",
       " 'class_weight': 'balanced',\n",
       " 'coef0': 0.0,\n",
       " 'decision_function_shape': 'ovr',\n",
       " 'degree': 3,\n",
       " 'gamma': 'scale',\n",
       " 'kernel': 'rbf',\n",
       " 'max_iter': -1,\n",
       " 'probability': False,\n",
       " 'random_state': None,\n",
       " 'shrinking': True,\n",
       " 'tol': 0.001,\n",
       " 'verbose': False}"
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "# loaded_clf.get_params(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [27357, 1]",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-18fa53e1ea7f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"TN:{}, FP:{}, FN:{}, TP:{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m     \"\"\"\n\u001b[1;32m--> 296\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not supported\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \"\"\"\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 263\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [27357, 1]"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = sklearn.metrics.confusion_matrix(y_test, [y_pred]).ravel()\n",
    "print(\"TN:{}, FP:{}, FN:{}, TP:{}\".format(tn, fp, fn, tp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "specificity= 0.8802296964725185 , sensitivity= 0.9897959183673469\n"
     ]
    }
   ],
   "source": [
    "specificity=(tn)/(tn+fp)\n",
    "sensitivity=(tp)/(tp+fn)\n",
    "print('specificity= {} , sensitivity= {}'.format(specificity, sensitivity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X size =(14584, 208), y size = (14584,)\ninterictal size =(12188, 209), preictal size = (975, 209)\ntrain size =(11846, 208), test size = (1317, 208)\nTraining Accuracy: 87%\nTesting Accuracy: 88%\nTN:1073, FP:146, FN:1, TP:97\nspecificity= 0.8802296964725185 , sensitivity= 0.9897959183673469\n"
     ]
    }
   ],
   "source": [
    "print('X size ={}, y size = {}'.format(X_shape, y_shape))\n",
    "print('interictal size ={}, preictal size = {}'.format(interictal_shape, preictal_shape))\n",
    "print('train size ={}, test size = {}'.format(X_train_shape, X_test_shape))\n",
    "\n",
    "print(\"Training Accuracy: %d\"%(trainAcc*100)+\"%\")\n",
    "print(\"Testing Accuracy: %d\"%(testAcc *100)+\"%\")\n",
    "print(\"TN:{}, FP:{}, FN:{}, TP:{}\".format(tn, fp, fn, tp))\n",
    "print('specificity= {} , sensitivity= {}'.format(specificity, sensitivity))"
   ]
  },
  {
   "source": [
    "### grid search"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numpy import mean\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.model_selection import RepeatedStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance = [{0:10,2:1}, {0:1,2:1}, {0:1,2:10}, {0:1,2:50}, {0:1,2:100}]\n",
    "# costs=[1.0,10.0,100.0]\n",
    "# param_grid = dict(C=costs, class_weight=balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
    "# grid = GridSearchCV(estimator=clf, param_grid=param_grid, n_jobs=-1, cv=cv, scoring='f1_weighted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best: 0.897876 using {'C': 100.0, 'class_weight': {0: 1, 2: 10}}\n0.890373 (0.000291) with: {'C': 1.0, 'class_weight': {0: 10, 2: 1}}\n0.890373 (0.000291) with: {'C': 1.0, 'class_weight': {0: 1, 2: 1}}\n0.749362 (0.010585) with: {'C': 1.0, 'class_weight': {0: 1, 2: 10}}\n0.651175 (0.005219) with: {'C': 1.0, 'class_weight': {0: 1, 2: 50}}\n0.631188 (0.009186) with: {'C': 1.0, 'class_weight': {0: 1, 2: 100}}\n0.890373 (0.000291) with: {'C': 10.0, 'class_weight': {0: 10, 2: 1}}\n0.890373 (0.000291) with: {'C': 10.0, 'class_weight': {0: 1, 2: 1}}\n0.855444 (0.006939) with: {'C': 10.0, 'class_weight': {0: 1, 2: 10}}\n0.784134 (0.010316) with: {'C': 10.0, 'class_weight': {0: 1, 2: 50}}\n0.750769 (0.007781) with: {'C': 10.0, 'class_weight': {0: 1, 2: 100}}\n0.890373 (0.000291) with: {'C': 100.0, 'class_weight': {0: 10, 2: 1}}\n0.890787 (0.000550) with: {'C': 100.0, 'class_weight': {0: 1, 2: 1}}\n0.897876 (0.006056) with: {'C': 100.0, 'class_weight': {0: 1, 2: 10}}\n0.849596 (0.008939) with: {'C': 100.0, 'class_weight': {0: 1, 2: 50}}\n0.821586 (0.009832) with: {'C': 100.0, 'class_weight': {0: 1, 2: 100}}\n"
     ]
    }
   ],
   "source": [
    "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "# means = grid_result.cv_results_['mean_test_score']\n",
    "# stds = grid_result.cv_results_['std_test_score']\n",
    "# params = grid_result.cv_results_['params']\n",
    "# for mean, stdev, param in zip(means, stds, params):\n",
    "#     print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}