{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37364bittfgpucondac099bc5da9054a33945af826af1262b9",
   "display_name": "Python 3.7.3 64-bit ('tf-gpu': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "4b3fe4e430614ae3e36b8912aa67966595d32d796c9f5e46d3c96e8e35c078a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import numpy.fft as fft\n",
    "import mne\n",
    "from numpy import save, load\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.utils import np_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gpus available [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "gpus=tf.config.list_physical_devices()\n",
    "print(\"gpus available\", gpus)\n",
    "# tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "# print(tf.test.gpu_device_name())"
   ]
  },
  {
   "source": [
    "### read dataset X and Y"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "72\n",
      "(12968, 208)\n",
      "78\n",
      "(27189, 208)\n",
      "38\n",
      "(51560, 208)\n",
      "70\n",
      "(64000, 208)\n",
      "42\n",
      "(71760, 208)\n"
     ]
    }
   ],
   "source": [
    "subject_id=2\n",
    "base_path = \"features_notwelch/\"\n",
    "# edf_file_names = sorted(glob.glob(os.path.join(base_path, \"data_chb01/*.npy\".format(subject_id))))\n",
    "# files=len(edf_file_names)\n",
    "X=load('features_notwelch/data_chb{:02d}/features_{}_00.npy'.format(subject_id,subject_id))\n",
    "y=load('features_notwelch/data_chb{:02d}/targets_{}_00.npy'.format(subject_id,subject_id))\n",
    "for subject_id in (2,5,9,11,12):\n",
    "    edf_file_names = sorted(glob.glob(os.path.join(base_path, \"data_chb{:02d}/*.npy\".format(subject_id))))\n",
    "    files=len(edf_file_names)\n",
    "    start=0\n",
    "    if subject_id==1:\n",
    "        start=1\n",
    "    for fileno in range(start, files//2):\n",
    "        X=np.concatenate((X, load('features_notwelch/data_chb{:02d}/features_{}_{:02d}.npy'.format(subject_id, subject_id, fileno))))\n",
    "        y=np.concatenate((y, load('features_notwelch/data_chb{:02d}/targets_{}_{:02d}.npy'.format(subject_id,subject_id, fileno))))\n",
    "    print(files)\n",
    "    print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(71760, 208) (71760,)\n"
     ]
    }
   ],
   "source": [
    "X_shape, y_shape = X.shape, y.shape\n",
    "print(X_shape, y_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_column_names():\n",
    "    channel_order= {'FP1-F7':0, 'F7-T7':1, 'T7-P7':2, 'P7-O1':3, 'FP1-F3':4, 'F3-C3':5, 'C3-P3':6, 'P3-O1':7, 'FP2-F4':8, 'F4-C4':9, 'C4-P4':10, 'P4-O2':11, 'FP2-F8':12, 'F8-T8':13, 'T8-P8-0':14, 'P8-O2':15, 'FZ-CZ':16, 'CZ-PZ':17, 'P7-T7':18, 'T7-FT9':19, 'FT9-FT10':20, 'FT10-T8':21 ,'T8-P8-1':22 }\n",
    "    col_names=[]\n",
    "    for ch in channel_order:\n",
    "        # print(ch)\n",
    "        for i in range(0,9):\n",
    "            col_names.append(str(ch)+'-'+str(i))\n",
    "    # col_names.append('rms')\n",
    "    # col_names.append('sum')\n",
    "    col_names.append('patient')\n",
    "    # print(col_names.shape)\n",
    "    return col_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(71760, 209)\n(71759, 209)\n"
     ]
    }
   ],
   "source": [
    "df=pd.DataFrame(data=X, columns=generate_column_names())\n",
    "df['target']=y\n",
    "print(df.shape)\n",
    "df=df.dropna()\n",
    "print(df.shape)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['seizure_number']=0\n",
    "# seizure_number=1\n",
    "# for i in range(1, len(df)):\n",
    "#     # if df.iloc[i]['target']==2 and df.iloc[i-1]['target']==2:\n",
    "#     #     df.iloc[i]['seizure_number']=df.iloc[i-1]['seizure_number']\n",
    "#     if (df.iloc[i]['target']==2):\n",
    "#         df.at[i,'seizure_number']=seizure_number\n",
    "#     if (df.iloc[i]['target']!=2 and df.iloc[i-1]['target']==2 ):\n",
    "#         seizure_number+=1\n",
    "# test_df=pd.DataFrame()\n",
    "# for patient in pd.unique(df['patient']):\n",
    "#     # patdf=df[df[207]==patient]\n",
    "#     last_seizure=max(df[df['patient']==patient]['seizure_number'])\n",
    "#     test_df = test_df.append(df[(df['patient']==patient) & (df['seizure_number']==last_seizure)], ignore_index=False)\n",
    "# test_df\n",
    "# # df.to_csv(r'df2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       FP1-F7-0  FP1-F7-1  FP1-F7-2  FP1-F7-3  FP1-F7-4  FP1-F7-5  FP1-F7-6  \\\n",
       "0      0.018578  0.095241  0.026204  0.026413  0.267436  0.122472  0.402273   \n",
       "1      0.017195  0.116917  0.028268  0.020735  0.287858  0.113165  0.369562   \n",
       "2      0.023806  0.160434  0.033086  0.027919  0.314111  0.112796  0.305782   \n",
       "3      0.024970  0.143778  0.034336  0.036952  0.303799  0.135452  0.329680   \n",
       "4      0.002849  0.048261  0.002897  0.003678  0.153665  0.130988  0.510873   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "71754  0.014313  0.153195  0.006152  0.014744  0.287621  0.075952  0.289818   \n",
       "71755  0.013912  0.131002  0.005290  0.012155  0.273308  0.073352  0.323711   \n",
       "71756  0.022538  0.186055  0.008216  0.021497  0.289533  0.059521  0.311911   \n",
       "71757  0.047645  0.311022  0.020933  0.061205  0.291337  0.052105  0.178151   \n",
       "71758  0.045307  0.316278  0.023467  0.076377  0.281629  0.050850  0.152615   \n",
       "\n",
       "       FP1-F7-7      FP1-F7-8   F7-T7-0  ...  T8-P8-1-2  T8-P8-1-3  T8-P8-1-4  \\\n",
       "0      0.072887   1553.302246  0.014406  ...   0.020839   0.067815   0.342813   \n",
       "1      0.061889   2126.991699  0.011974  ...   0.008063   0.016239   0.368241   \n",
       "2      0.049759   1298.781006  0.011973  ...   0.004080   0.009234   0.416091   \n",
       "3      0.059968    754.909790  0.007608  ...   0.002542   0.005959   0.413690   \n",
       "4      0.140414  12543.096680  0.002975  ...   0.001261   0.004485   0.265586   \n",
       "...         ...           ...       ...  ...        ...        ...        ...   \n",
       "71754  0.106538   7432.621582  0.042097  ...   0.002883   0.010063   0.412930   \n",
       "71755  0.118133   8756.443359  0.036799  ...   0.003953   0.008558   0.271229   \n",
       "71756  0.096910   4677.033691  0.043397  ...   0.010345   0.012555   0.241365   \n",
       "71757  0.059963   1808.921387  0.048682  ...   0.019911   0.028009   0.315189   \n",
       "71758  0.062646   1767.698608  0.048571  ...   0.005629   0.015216   0.280535   \n",
       "\n",
       "       T8-P8-1-5  T8-P8-1-6  T8-P8-1-7     T8-P8-1-8  patient  target  \\\n",
       "0       0.082369   0.219504   0.057476   2179.300537      2.0       0   \n",
       "1       0.079414   0.265690   0.065642   5608.150391      2.0       0   \n",
       "2       0.060620   0.254375   0.063313  12217.144531      2.0       0   \n",
       "3       0.065602   0.252079   0.058523  14063.621094      2.0       0   \n",
       "4       0.088062   0.393971   0.085730  18503.945312      2.0       0   \n",
       "...          ...        ...        ...           ...      ...     ...   \n",
       "71754   0.066247   0.244435   0.086542   2857.702148     12.0       3   \n",
       "71755   0.079665   0.265976   0.107630   1545.392578     12.0       3   \n",
       "71756   0.096627   0.233801   0.089307    952.947632     12.0       3   \n",
       "71757   0.075089   0.202171   0.086369    561.303345     12.0       3   \n",
       "71758   0.078514   0.196172   0.083562   1230.518799     12.0       3   \n",
       "\n",
       "       seizure_number  \n",
       "0                   0  \n",
       "1                   0  \n",
       "2                   0  \n",
       "3                   0  \n",
       "4                   0  \n",
       "...               ...  \n",
       "71754               0  \n",
       "71755               0  \n",
       "71756               0  \n",
       "71757               0  \n",
       "71758               0  \n",
       "\n",
       "[71759 rows x 210 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>FP1-F7-0</th>\n      <th>FP1-F7-1</th>\n      <th>FP1-F7-2</th>\n      <th>FP1-F7-3</th>\n      <th>FP1-F7-4</th>\n      <th>FP1-F7-5</th>\n      <th>FP1-F7-6</th>\n      <th>FP1-F7-7</th>\n      <th>FP1-F7-8</th>\n      <th>F7-T7-0</th>\n      <th>...</th>\n      <th>T8-P8-1-2</th>\n      <th>T8-P8-1-3</th>\n      <th>T8-P8-1-4</th>\n      <th>T8-P8-1-5</th>\n      <th>T8-P8-1-6</th>\n      <th>T8-P8-1-7</th>\n      <th>T8-P8-1-8</th>\n      <th>patient</th>\n      <th>target</th>\n      <th>seizure_number</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.018578</td>\n      <td>0.095241</td>\n      <td>0.026204</td>\n      <td>0.026413</td>\n      <td>0.267436</td>\n      <td>0.122472</td>\n      <td>0.402273</td>\n      <td>0.072887</td>\n      <td>1553.302246</td>\n      <td>0.014406</td>\n      <td>...</td>\n      <td>0.020839</td>\n      <td>0.067815</td>\n      <td>0.342813</td>\n      <td>0.082369</td>\n      <td>0.219504</td>\n      <td>0.057476</td>\n      <td>2179.300537</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.017195</td>\n      <td>0.116917</td>\n      <td>0.028268</td>\n      <td>0.020735</td>\n      <td>0.287858</td>\n      <td>0.113165</td>\n      <td>0.369562</td>\n      <td>0.061889</td>\n      <td>2126.991699</td>\n      <td>0.011974</td>\n      <td>...</td>\n      <td>0.008063</td>\n      <td>0.016239</td>\n      <td>0.368241</td>\n      <td>0.079414</td>\n      <td>0.265690</td>\n      <td>0.065642</td>\n      <td>5608.150391</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.023806</td>\n      <td>0.160434</td>\n      <td>0.033086</td>\n      <td>0.027919</td>\n      <td>0.314111</td>\n      <td>0.112796</td>\n      <td>0.305782</td>\n      <td>0.049759</td>\n      <td>1298.781006</td>\n      <td>0.011973</td>\n      <td>...</td>\n      <td>0.004080</td>\n      <td>0.009234</td>\n      <td>0.416091</td>\n      <td>0.060620</td>\n      <td>0.254375</td>\n      <td>0.063313</td>\n      <td>12217.144531</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.024970</td>\n      <td>0.143778</td>\n      <td>0.034336</td>\n      <td>0.036952</td>\n      <td>0.303799</td>\n      <td>0.135452</td>\n      <td>0.329680</td>\n      <td>0.059968</td>\n      <td>754.909790</td>\n      <td>0.007608</td>\n      <td>...</td>\n      <td>0.002542</td>\n      <td>0.005959</td>\n      <td>0.413690</td>\n      <td>0.065602</td>\n      <td>0.252079</td>\n      <td>0.058523</td>\n      <td>14063.621094</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.002849</td>\n      <td>0.048261</td>\n      <td>0.002897</td>\n      <td>0.003678</td>\n      <td>0.153665</td>\n      <td>0.130988</td>\n      <td>0.510873</td>\n      <td>0.140414</td>\n      <td>12543.096680</td>\n      <td>0.002975</td>\n      <td>...</td>\n      <td>0.001261</td>\n      <td>0.004485</td>\n      <td>0.265586</td>\n      <td>0.088062</td>\n      <td>0.393971</td>\n      <td>0.085730</td>\n      <td>18503.945312</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>71754</th>\n      <td>0.014313</td>\n      <td>0.153195</td>\n      <td>0.006152</td>\n      <td>0.014744</td>\n      <td>0.287621</td>\n      <td>0.075952</td>\n      <td>0.289818</td>\n      <td>0.106538</td>\n      <td>7432.621582</td>\n      <td>0.042097</td>\n      <td>...</td>\n      <td>0.002883</td>\n      <td>0.010063</td>\n      <td>0.412930</td>\n      <td>0.066247</td>\n      <td>0.244435</td>\n      <td>0.086542</td>\n      <td>2857.702148</td>\n      <td>12.0</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>71755</th>\n      <td>0.013912</td>\n      <td>0.131002</td>\n      <td>0.005290</td>\n      <td>0.012155</td>\n      <td>0.273308</td>\n      <td>0.073352</td>\n      <td>0.323711</td>\n      <td>0.118133</td>\n      <td>8756.443359</td>\n      <td>0.036799</td>\n      <td>...</td>\n      <td>0.003953</td>\n      <td>0.008558</td>\n      <td>0.271229</td>\n      <td>0.079665</td>\n      <td>0.265976</td>\n      <td>0.107630</td>\n      <td>1545.392578</td>\n      <td>12.0</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>71756</th>\n      <td>0.022538</td>\n      <td>0.186055</td>\n      <td>0.008216</td>\n      <td>0.021497</td>\n      <td>0.289533</td>\n      <td>0.059521</td>\n      <td>0.311911</td>\n      <td>0.096910</td>\n      <td>4677.033691</td>\n      <td>0.043397</td>\n      <td>...</td>\n      <td>0.010345</td>\n      <td>0.012555</td>\n      <td>0.241365</td>\n      <td>0.096627</td>\n      <td>0.233801</td>\n      <td>0.089307</td>\n      <td>952.947632</td>\n      <td>12.0</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>71757</th>\n      <td>0.047645</td>\n      <td>0.311022</td>\n      <td>0.020933</td>\n      <td>0.061205</td>\n      <td>0.291337</td>\n      <td>0.052105</td>\n      <td>0.178151</td>\n      <td>0.059963</td>\n      <td>1808.921387</td>\n      <td>0.048682</td>\n      <td>...</td>\n      <td>0.019911</td>\n      <td>0.028009</td>\n      <td>0.315189</td>\n      <td>0.075089</td>\n      <td>0.202171</td>\n      <td>0.086369</td>\n      <td>561.303345</td>\n      <td>12.0</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>71758</th>\n      <td>0.045307</td>\n      <td>0.316278</td>\n      <td>0.023467</td>\n      <td>0.076377</td>\n      <td>0.281629</td>\n      <td>0.050850</td>\n      <td>0.152615</td>\n      <td>0.062646</td>\n      <td>1767.698608</td>\n      <td>0.048571</td>\n      <td>...</td>\n      <td>0.005629</td>\n      <td>0.015216</td>\n      <td>0.280535</td>\n      <td>0.078514</td>\n      <td>0.196172</td>\n      <td>0.083562</td>\n      <td>1230.518799</td>\n      <td>12.0</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>71759 rows × 210 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 220
    }
   ],
   "source": [
    "df['seizure_number']=0\n",
    "\n",
    "seizure_number=1\n",
    "for i in range(1, len(df)):\n",
    "    # if df.iloc[i]['target']==2 and df.iloc[i-1]['target']==2:\n",
    "    #     df.iloc[i]['seizure_number']=df.iloc[i-1]['seizure_number']\n",
    "    if df.iloc[i]['target']==2:\n",
    "        df.at[i,'seizure_number']=seizure_number\n",
    "    if (df.iloc[i]['target']!=2 and df.iloc[i-1]['target']==2 ):\n",
    "        seizure_number+=1\n",
    "df\n",
    "# df.to_csv(r'df.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.0\n5.0\n9.0\n11.0\n12.0\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       FP1-F7-0  FP1-F7-1  FP1-F7-2  FP1-F7-3  FP1-F7-4  FP1-F7-5  FP1-F7-6  \\\n",
       "7162   0.024722  0.167940  0.022232  0.040775  0.372914  0.077367  0.253885   \n",
       "7163   0.011673  0.140104  0.005736  0.013079  0.467539  0.068067  0.237187   \n",
       "7164   0.012789  0.161469  0.016808  0.020162  0.463838  0.060573  0.216059   \n",
       "7165   0.026937  0.173863  0.068735  0.064965  0.363154  0.056674  0.210230   \n",
       "7166   0.021215  0.137705  0.071709  0.057182  0.398063  0.055010  0.224288   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "71573  0.057434  0.449707  0.033225  0.051070  0.433269  0.009048  0.022220   \n",
       "71574  0.055667  0.435668  0.037922  0.050661  0.435502  0.009454  0.022743   \n",
       "71575  0.062079  0.444765  0.043678  0.049211  0.426007  0.008424  0.021301   \n",
       "71576  0.053513  0.503538  0.030657  0.035249  0.403018  0.006758  0.017817   \n",
       "71577  0.052255  0.475701  0.021347  0.025140  0.439827  0.008385  0.018634   \n",
       "\n",
       "       FP1-F7-7     FP1-F7-8   F7-T7-0  ...  T8-P8-1-2  T8-P8-1-3  T8-P8-1-4  \\\n",
       "7162   0.053518  1946.139648  0.023040  ...   0.005336   0.014324   0.345691   \n",
       "7163   0.047278  4959.291992  0.029791  ...   0.007012   0.022235   0.356219   \n",
       "7164   0.040374  4813.878418  0.015255  ...   0.004499   0.014204   0.294616   \n",
       "7165   0.039481  2711.432373  0.013499  ...   0.004150   0.010867   0.279594   \n",
       "7166   0.041962  3424.862793  0.015650  ...   0.007664   0.013128   0.336973   \n",
       "...         ...          ...       ...  ...        ...        ...        ...   \n",
       "71573  0.004940   413.782379  0.058471  ...   0.160693   0.272260   0.042583   \n",
       "71574  0.005223   379.738708  0.058253  ...   0.141379   0.150678   0.057322   \n",
       "71575  0.005031   392.124817  0.057531  ...   0.161199   0.189323   0.056296   \n",
       "71576  0.003927   497.231781  0.049436  ...   0.139491   0.192732   0.052969   \n",
       "71577  0.003361   591.865540  0.048061  ...   0.136800   0.186728   0.043668   \n",
       "\n",
       "       T8-P8-1-5  T8-P8-1-6  T8-P8-1-7    T8-P8-1-8  patient  target  \\\n",
       "7162    0.072008   0.255091   0.056759  3028.504395      2.0       2   \n",
       "7163    0.065431   0.226064   0.044866  1992.042847      2.0       2   \n",
       "7164    0.076901   0.262006   0.047901  2765.090088      2.0       2   \n",
       "7165    0.081538   0.272143   0.058216  4110.312988      2.0       2   \n",
       "7166    0.069166   0.258830   0.061734  5443.549805      2.0       2   \n",
       "...          ...        ...        ...          ...      ...     ...   \n",
       "71573   0.000433   0.001810   0.001268   273.556274     12.0       2   \n",
       "71574   0.000523   0.001846   0.001473   226.755234     12.0       2   \n",
       "71575   0.000672   0.003524   0.001930   206.779114     12.0       2   \n",
       "71576   0.000643   0.003340   0.001971   221.903259     12.0       2   \n",
       "71577   0.000433   0.001655   0.001530   255.963150     12.0       2   \n",
       "\n",
       "       seizure_number  \n",
       "7162                3  \n",
       "7163                3  \n",
       "7164                3  \n",
       "7165                3  \n",
       "7166                3  \n",
       "...               ...  \n",
       "71573              39  \n",
       "71574              39  \n",
       "71575              39  \n",
       "71576              39  \n",
       "71577              39  \n",
       "\n",
       "[281 rows x 210 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>FP1-F7-0</th>\n      <th>FP1-F7-1</th>\n      <th>FP1-F7-2</th>\n      <th>FP1-F7-3</th>\n      <th>FP1-F7-4</th>\n      <th>FP1-F7-5</th>\n      <th>FP1-F7-6</th>\n      <th>FP1-F7-7</th>\n      <th>FP1-F7-8</th>\n      <th>F7-T7-0</th>\n      <th>...</th>\n      <th>T8-P8-1-2</th>\n      <th>T8-P8-1-3</th>\n      <th>T8-P8-1-4</th>\n      <th>T8-P8-1-5</th>\n      <th>T8-P8-1-6</th>\n      <th>T8-P8-1-7</th>\n      <th>T8-P8-1-8</th>\n      <th>patient</th>\n      <th>target</th>\n      <th>seizure_number</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>7162</th>\n      <td>0.024722</td>\n      <td>0.167940</td>\n      <td>0.022232</td>\n      <td>0.040775</td>\n      <td>0.372914</td>\n      <td>0.077367</td>\n      <td>0.253885</td>\n      <td>0.053518</td>\n      <td>1946.139648</td>\n      <td>0.023040</td>\n      <td>...</td>\n      <td>0.005336</td>\n      <td>0.014324</td>\n      <td>0.345691</td>\n      <td>0.072008</td>\n      <td>0.255091</td>\n      <td>0.056759</td>\n      <td>3028.504395</td>\n      <td>2.0</td>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>7163</th>\n      <td>0.011673</td>\n      <td>0.140104</td>\n      <td>0.005736</td>\n      <td>0.013079</td>\n      <td>0.467539</td>\n      <td>0.068067</td>\n      <td>0.237187</td>\n      <td>0.047278</td>\n      <td>4959.291992</td>\n      <td>0.029791</td>\n      <td>...</td>\n      <td>0.007012</td>\n      <td>0.022235</td>\n      <td>0.356219</td>\n      <td>0.065431</td>\n      <td>0.226064</td>\n      <td>0.044866</td>\n      <td>1992.042847</td>\n      <td>2.0</td>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>7164</th>\n      <td>0.012789</td>\n      <td>0.161469</td>\n      <td>0.016808</td>\n      <td>0.020162</td>\n      <td>0.463838</td>\n      <td>0.060573</td>\n      <td>0.216059</td>\n      <td>0.040374</td>\n      <td>4813.878418</td>\n      <td>0.015255</td>\n      <td>...</td>\n      <td>0.004499</td>\n      <td>0.014204</td>\n      <td>0.294616</td>\n      <td>0.076901</td>\n      <td>0.262006</td>\n      <td>0.047901</td>\n      <td>2765.090088</td>\n      <td>2.0</td>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>7165</th>\n      <td>0.026937</td>\n      <td>0.173863</td>\n      <td>0.068735</td>\n      <td>0.064965</td>\n      <td>0.363154</td>\n      <td>0.056674</td>\n      <td>0.210230</td>\n      <td>0.039481</td>\n      <td>2711.432373</td>\n      <td>0.013499</td>\n      <td>...</td>\n      <td>0.004150</td>\n      <td>0.010867</td>\n      <td>0.279594</td>\n      <td>0.081538</td>\n      <td>0.272143</td>\n      <td>0.058216</td>\n      <td>4110.312988</td>\n      <td>2.0</td>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>7166</th>\n      <td>0.021215</td>\n      <td>0.137705</td>\n      <td>0.071709</td>\n      <td>0.057182</td>\n      <td>0.398063</td>\n      <td>0.055010</td>\n      <td>0.224288</td>\n      <td>0.041962</td>\n      <td>3424.862793</td>\n      <td>0.015650</td>\n      <td>...</td>\n      <td>0.007664</td>\n      <td>0.013128</td>\n      <td>0.336973</td>\n      <td>0.069166</td>\n      <td>0.258830</td>\n      <td>0.061734</td>\n      <td>5443.549805</td>\n      <td>2.0</td>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>71573</th>\n      <td>0.057434</td>\n      <td>0.449707</td>\n      <td>0.033225</td>\n      <td>0.051070</td>\n      <td>0.433269</td>\n      <td>0.009048</td>\n      <td>0.022220</td>\n      <td>0.004940</td>\n      <td>413.782379</td>\n      <td>0.058471</td>\n      <td>...</td>\n      <td>0.160693</td>\n      <td>0.272260</td>\n      <td>0.042583</td>\n      <td>0.000433</td>\n      <td>0.001810</td>\n      <td>0.001268</td>\n      <td>273.556274</td>\n      <td>12.0</td>\n      <td>2</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>71574</th>\n      <td>0.055667</td>\n      <td>0.435668</td>\n      <td>0.037922</td>\n      <td>0.050661</td>\n      <td>0.435502</td>\n      <td>0.009454</td>\n      <td>0.022743</td>\n      <td>0.005223</td>\n      <td>379.738708</td>\n      <td>0.058253</td>\n      <td>...</td>\n      <td>0.141379</td>\n      <td>0.150678</td>\n      <td>0.057322</td>\n      <td>0.000523</td>\n      <td>0.001846</td>\n      <td>0.001473</td>\n      <td>226.755234</td>\n      <td>12.0</td>\n      <td>2</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>71575</th>\n      <td>0.062079</td>\n      <td>0.444765</td>\n      <td>0.043678</td>\n      <td>0.049211</td>\n      <td>0.426007</td>\n      <td>0.008424</td>\n      <td>0.021301</td>\n      <td>0.005031</td>\n      <td>392.124817</td>\n      <td>0.057531</td>\n      <td>...</td>\n      <td>0.161199</td>\n      <td>0.189323</td>\n      <td>0.056296</td>\n      <td>0.000672</td>\n      <td>0.003524</td>\n      <td>0.001930</td>\n      <td>206.779114</td>\n      <td>12.0</td>\n      <td>2</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>71576</th>\n      <td>0.053513</td>\n      <td>0.503538</td>\n      <td>0.030657</td>\n      <td>0.035249</td>\n      <td>0.403018</td>\n      <td>0.006758</td>\n      <td>0.017817</td>\n      <td>0.003927</td>\n      <td>497.231781</td>\n      <td>0.049436</td>\n      <td>...</td>\n      <td>0.139491</td>\n      <td>0.192732</td>\n      <td>0.052969</td>\n      <td>0.000643</td>\n      <td>0.003340</td>\n      <td>0.001971</td>\n      <td>221.903259</td>\n      <td>12.0</td>\n      <td>2</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>71577</th>\n      <td>0.052255</td>\n      <td>0.475701</td>\n      <td>0.021347</td>\n      <td>0.025140</td>\n      <td>0.439827</td>\n      <td>0.008385</td>\n      <td>0.018634</td>\n      <td>0.003361</td>\n      <td>591.865540</td>\n      <td>0.048061</td>\n      <td>...</td>\n      <td>0.136800</td>\n      <td>0.186728</td>\n      <td>0.043668</td>\n      <td>0.000433</td>\n      <td>0.001655</td>\n      <td>0.001530</td>\n      <td>255.963150</td>\n      <td>12.0</td>\n      <td>2</td>\n      <td>39</td>\n    </tr>\n  </tbody>\n</table>\n<p>281 rows × 210 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 221
    }
   ],
   "source": [
    "test_df=pd.DataFrame()\n",
    "for patient in pd.unique(df['patient']):\n",
    "    # patdf=df[df[207]==patient]\n",
    "    print(patient)\n",
    "    last_seizure=max(df[df['patient']==patient]['seizure_number'])\n",
    "    test_df = test_df.append(df[(df['patient']==patient) & (df['seizure_number']==last_seizure)], ignore_index=False)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [FP1-F7-0, FP1-F7-1, FP1-F7-2, FP1-F7-3, FP1-F7-4, FP1-F7-5, FP1-F7-6, FP1-F7-7, FP1-F7-8, F7-T7-0, F7-T7-1, F7-T7-2, F7-T7-3, F7-T7-4, F7-T7-5, F7-T7-6, F7-T7-7, F7-T7-8, T7-P7-0, T7-P7-1, T7-P7-2, T7-P7-3, T7-P7-4, T7-P7-5, T7-P7-6, T7-P7-7, T7-P7-8, P7-O1-0, P7-O1-1, P7-O1-2, P7-O1-3, P7-O1-4, P7-O1-5, P7-O1-6, P7-O1-7, P7-O1-8, FP1-F3-0, FP1-F3-1, FP1-F3-2, FP1-F3-3, FP1-F3-4, FP1-F3-5, FP1-F3-6, FP1-F3-7, FP1-F3-8, F3-C3-0, F3-C3-1, F3-C3-2, F3-C3-3, F3-C3-4, F3-C3-5, F3-C3-6, F3-C3-7, F3-C3-8, C3-P3-0, C3-P3-1, C3-P3-2, C3-P3-3, C3-P3-4, C3-P3-5, C3-P3-6, C3-P3-7, C3-P3-8, P3-O1-0, P3-O1-1, P3-O1-2, P3-O1-3, P3-O1-4, P3-O1-5, P3-O1-6, P3-O1-7, P3-O1-8, FP2-F4-0, FP2-F4-1, FP2-F4-2, FP2-F4-3, FP2-F4-4, FP2-F4-5, FP2-F4-6, FP2-F4-7, FP2-F4-8, F4-C4-0, F4-C4-1, F4-C4-2, F4-C4-3, F4-C4-4, F4-C4-5, F4-C4-6, F4-C4-7, F4-C4-8, C4-P4-0, C4-P4-1, C4-P4-2, C4-P4-3, C4-P4-4, C4-P4-5, C4-P4-6, C4-P4-7, C4-P4-8, P4-O2-0, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 210 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>FP1-F7-0</th>\n      <th>FP1-F7-1</th>\n      <th>FP1-F7-2</th>\n      <th>FP1-F7-3</th>\n      <th>FP1-F7-4</th>\n      <th>FP1-F7-5</th>\n      <th>FP1-F7-6</th>\n      <th>FP1-F7-7</th>\n      <th>FP1-F7-8</th>\n      <th>F7-T7-0</th>\n      <th>...</th>\n      <th>T8-P8-1-2</th>\n      <th>T8-P8-1-3</th>\n      <th>T8-P8-1-4</th>\n      <th>T8-P8-1-5</th>\n      <th>T8-P8-1-6</th>\n      <th>T8-P8-1-7</th>\n      <th>T8-P8-1-8</th>\n      <th>patient</th>\n      <th>target</th>\n      <th>seizure_number</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n<p>0 rows × 210 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 222
    }
   ],
   "source": [
    "test_df[test_df['target']!=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df.to_csv(r'test_df.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.index.isin(test_df.index) == False]"
   ]
  },
  {
   "source": [
    "### train test split"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(12494, 210) (1519, 210)\n"
     ]
    }
   ],
   "source": [
    "df_interictal=df[df['target']==0]\n",
    "# print(df_interictal.shape)\n",
    "df_interictal=df_interictal.sample(frac=0.2)\n",
    "df_preictal=df[df['target']==2]\n",
    "df_preictal = df_preictal.replace({'target': 2}, 1)\n",
    "test_df = test_df.replace({'target': 2}, 1)\n",
    "interictal_shape, preictal_shape = df_interictal.shape, df_preictal.shape\n",
    "print(df_interictal.shape, df_preictal.shape)\n",
    "\n",
    "# df_interictal=df[df['target']==0]\n",
    "# df_interictal=df_interictal.sample(frac=0.2)\n",
    "# df_preictal=df[df['target']==2]\n",
    "# interictal_shape, preictal_shape = df_interictal.shape, df_preictal.shape\n",
    "# interictal_shape= df_interictal.shape\n",
    "\n",
    "# print(df_interictal.shape,  df_preictal.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_interictal=np.array(df_interictal[df.columns[0:-2]]).astype('float32')\n",
    "# y_interictal=np.array(df_interictal['target']).astype('float32')\n",
    "# X_preictal_train  =np.array(df_preictal[df.columns[:-2]]).astype('float32')\n",
    "# y_preictal_train  =np.array(df_preictal['target']).astype('float32')\n",
    "X_interictal=np.array(df_interictal[df.columns[:-2]]).astype('float32')\n",
    "y_interictal=np.array(df_interictal['target']).astype('float32')\n",
    "X_preictal_train  =np.array(df_preictal[df.columns[:-2]]).astype('float32')\n",
    "y_preictal_train  =np.array(df_preictal['target']).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(12494, 208)\n(12494,)\n(1519, 208)\n(1519,)\n"
     ]
    }
   ],
   "source": [
    "print(X_interictal.shape)\n",
    "print(y_interictal.shape)\n",
    "print(X_preictal_train.shape)\n",
    "print(y_preictal_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       FP1-F7-0  FP1-F7-1  FP1-F7-2  FP1-F7-3  FP1-F7-4  FP1-F7-5  FP1-F7-6  \\\n",
       "45972  0.004331  0.074158  0.002722  0.003501  0.175945  0.080066  0.442735   \n",
       "25143  0.097475  0.264733  0.150336  0.146393  0.297588  0.032300  0.095158   \n",
       "29553  0.013507  0.106932  0.012695  0.010930  0.186941  0.079720  0.394058   \n",
       "56595  0.002107  0.118466  0.002050  0.002152  0.471416  0.078310  0.265949   \n",
       "23658  0.013228  0.139170  0.079380  0.058290  0.227415  0.065873  0.297846   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "19865  0.036160  0.180962  0.123352  0.093771  0.329097  0.042889  0.187185   \n",
       "2350   0.014446  0.141418  0.032889  0.025948  0.258065  0.077724  0.333375   \n",
       "5618   0.010031  0.080618  0.020130  0.019936  0.220296  0.071255  0.379176   \n",
       "70609  0.080106  0.305004  0.104403  0.096742  0.289549  0.029230  0.100593   \n",
       "53227  0.016771  0.209713  0.010927  0.014668  0.372532  0.070309  0.251219   \n",
       "\n",
       "       FP1-F7-7      FP1-F7-8   F7-T7-0  ...  T8-P8-1-0  T8-P8-1-1  T8-P8-1-2  \\\n",
       "45972  0.154198  65254.500000  0.002624  ...   0.013827   0.201816   0.012313   \n",
       "25143  0.012931   1384.454834  0.100922  ...   0.170164   0.247275   0.200316   \n",
       "29553  0.159222  21730.957031  0.007660  ...   0.029254   0.160777   0.002360   \n",
       "56595  0.040193  35021.382812  0.016120  ...   0.044210   0.165352   0.010857   \n",
       "23658  0.101728   7768.779297  0.011053  ...   0.127749   0.153463   0.293808   \n",
       "...         ...           ...       ...  ...        ...        ...        ...   \n",
       "19865  0.053526   2206.135498  0.013610  ...   0.012585   0.197628   0.003468   \n",
       "2350   0.105910   3483.398438  0.005409  ...   0.015250   0.209534   0.003944   \n",
       "5618   0.155025   9990.822266  0.001578  ...   0.001123   0.094524   0.000090   \n",
       "70609  0.037282    482.822998  0.038154  ...   0.088845   0.477305   0.234229   \n",
       "53227  0.048426   2349.734619  0.013585  ...   0.015222   0.189199   0.003611   \n",
       "\n",
       "       T8-P8-1-3  T8-P8-1-4  T8-P8-1-5  T8-P8-1-6  T8-P8-1-7      T8-P8-1-8  \\\n",
       "45972   0.028431   0.263969   0.073814   0.294482   0.090222    9290.288086   \n",
       "25143   0.229634   0.177482   0.030912   0.076588   0.009490     507.993683   \n",
       "29553   0.012516   0.254266   0.068270   0.344016   0.108444   19499.537109   \n",
       "56595   0.036968   0.293260   0.067112   0.296906   0.082849    1210.145264   \n",
       "23658   0.368119   0.094259   0.013920   0.080695   0.020532    1282.029785   \n",
       "...          ...        ...        ...        ...        ...            ...   \n",
       "19865   0.004686   0.374663   0.058613   0.246230   0.074285   46744.343750   \n",
       "2350    0.010755   0.385972   0.073166   0.233872   0.053010    9562.818359   \n",
       "5618    0.000300   0.494216   0.066682   0.261224   0.055521  483009.375000   \n",
       "70609   0.247230   0.149067   0.001708   0.027211   0.009689    2295.086182   \n",
       "53227   0.009852   0.327217   0.077304   0.315849   0.044695    5007.505859   \n",
       "\n",
       "       patient  \n",
       "45972      9.0  \n",
       "25143      5.0  \n",
       "29553      9.0  \n",
       "56595     11.0  \n",
       "23658      5.0  \n",
       "...        ...  \n",
       "19865      5.0  \n",
       "2350       2.0  \n",
       "5618       2.0  \n",
       "70609     12.0  \n",
       "53227     11.0  \n",
       "\n",
       "[12494 rows x 208 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>FP1-F7-0</th>\n      <th>FP1-F7-1</th>\n      <th>FP1-F7-2</th>\n      <th>FP1-F7-3</th>\n      <th>FP1-F7-4</th>\n      <th>FP1-F7-5</th>\n      <th>FP1-F7-6</th>\n      <th>FP1-F7-7</th>\n      <th>FP1-F7-8</th>\n      <th>F7-T7-0</th>\n      <th>...</th>\n      <th>T8-P8-1-0</th>\n      <th>T8-P8-1-1</th>\n      <th>T8-P8-1-2</th>\n      <th>T8-P8-1-3</th>\n      <th>T8-P8-1-4</th>\n      <th>T8-P8-1-5</th>\n      <th>T8-P8-1-6</th>\n      <th>T8-P8-1-7</th>\n      <th>T8-P8-1-8</th>\n      <th>patient</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>45972</th>\n      <td>0.004331</td>\n      <td>0.074158</td>\n      <td>0.002722</td>\n      <td>0.003501</td>\n      <td>0.175945</td>\n      <td>0.080066</td>\n      <td>0.442735</td>\n      <td>0.154198</td>\n      <td>65254.500000</td>\n      <td>0.002624</td>\n      <td>...</td>\n      <td>0.013827</td>\n      <td>0.201816</td>\n      <td>0.012313</td>\n      <td>0.028431</td>\n      <td>0.263969</td>\n      <td>0.073814</td>\n      <td>0.294482</td>\n      <td>0.090222</td>\n      <td>9290.288086</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th>25143</th>\n      <td>0.097475</td>\n      <td>0.264733</td>\n      <td>0.150336</td>\n      <td>0.146393</td>\n      <td>0.297588</td>\n      <td>0.032300</td>\n      <td>0.095158</td>\n      <td>0.012931</td>\n      <td>1384.454834</td>\n      <td>0.100922</td>\n      <td>...</td>\n      <td>0.170164</td>\n      <td>0.247275</td>\n      <td>0.200316</td>\n      <td>0.229634</td>\n      <td>0.177482</td>\n      <td>0.030912</td>\n      <td>0.076588</td>\n      <td>0.009490</td>\n      <td>507.993683</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>29553</th>\n      <td>0.013507</td>\n      <td>0.106932</td>\n      <td>0.012695</td>\n      <td>0.010930</td>\n      <td>0.186941</td>\n      <td>0.079720</td>\n      <td>0.394058</td>\n      <td>0.159222</td>\n      <td>21730.957031</td>\n      <td>0.007660</td>\n      <td>...</td>\n      <td>0.029254</td>\n      <td>0.160777</td>\n      <td>0.002360</td>\n      <td>0.012516</td>\n      <td>0.254266</td>\n      <td>0.068270</td>\n      <td>0.344016</td>\n      <td>0.108444</td>\n      <td>19499.537109</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th>56595</th>\n      <td>0.002107</td>\n      <td>0.118466</td>\n      <td>0.002050</td>\n      <td>0.002152</td>\n      <td>0.471416</td>\n      <td>0.078310</td>\n      <td>0.265949</td>\n      <td>0.040193</td>\n      <td>35021.382812</td>\n      <td>0.016120</td>\n      <td>...</td>\n      <td>0.044210</td>\n      <td>0.165352</td>\n      <td>0.010857</td>\n      <td>0.036968</td>\n      <td>0.293260</td>\n      <td>0.067112</td>\n      <td>0.296906</td>\n      <td>0.082849</td>\n      <td>1210.145264</td>\n      <td>11.0</td>\n    </tr>\n    <tr>\n      <th>23658</th>\n      <td>0.013228</td>\n      <td>0.139170</td>\n      <td>0.079380</td>\n      <td>0.058290</td>\n      <td>0.227415</td>\n      <td>0.065873</td>\n      <td>0.297846</td>\n      <td>0.101728</td>\n      <td>7768.779297</td>\n      <td>0.011053</td>\n      <td>...</td>\n      <td>0.127749</td>\n      <td>0.153463</td>\n      <td>0.293808</td>\n      <td>0.368119</td>\n      <td>0.094259</td>\n      <td>0.013920</td>\n      <td>0.080695</td>\n      <td>0.020532</td>\n      <td>1282.029785</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>19865</th>\n      <td>0.036160</td>\n      <td>0.180962</td>\n      <td>0.123352</td>\n      <td>0.093771</td>\n      <td>0.329097</td>\n      <td>0.042889</td>\n      <td>0.187185</td>\n      <td>0.053526</td>\n      <td>2206.135498</td>\n      <td>0.013610</td>\n      <td>...</td>\n      <td>0.012585</td>\n      <td>0.197628</td>\n      <td>0.003468</td>\n      <td>0.004686</td>\n      <td>0.374663</td>\n      <td>0.058613</td>\n      <td>0.246230</td>\n      <td>0.074285</td>\n      <td>46744.343750</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>2350</th>\n      <td>0.014446</td>\n      <td>0.141418</td>\n      <td>0.032889</td>\n      <td>0.025948</td>\n      <td>0.258065</td>\n      <td>0.077724</td>\n      <td>0.333375</td>\n      <td>0.105910</td>\n      <td>3483.398438</td>\n      <td>0.005409</td>\n      <td>...</td>\n      <td>0.015250</td>\n      <td>0.209534</td>\n      <td>0.003944</td>\n      <td>0.010755</td>\n      <td>0.385972</td>\n      <td>0.073166</td>\n      <td>0.233872</td>\n      <td>0.053010</td>\n      <td>9562.818359</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>5618</th>\n      <td>0.010031</td>\n      <td>0.080618</td>\n      <td>0.020130</td>\n      <td>0.019936</td>\n      <td>0.220296</td>\n      <td>0.071255</td>\n      <td>0.379176</td>\n      <td>0.155025</td>\n      <td>9990.822266</td>\n      <td>0.001578</td>\n      <td>...</td>\n      <td>0.001123</td>\n      <td>0.094524</td>\n      <td>0.000090</td>\n      <td>0.000300</td>\n      <td>0.494216</td>\n      <td>0.066682</td>\n      <td>0.261224</td>\n      <td>0.055521</td>\n      <td>483009.375000</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>70609</th>\n      <td>0.080106</td>\n      <td>0.305004</td>\n      <td>0.104403</td>\n      <td>0.096742</td>\n      <td>0.289549</td>\n      <td>0.029230</td>\n      <td>0.100593</td>\n      <td>0.037282</td>\n      <td>482.822998</td>\n      <td>0.038154</td>\n      <td>...</td>\n      <td>0.088845</td>\n      <td>0.477305</td>\n      <td>0.234229</td>\n      <td>0.247230</td>\n      <td>0.149067</td>\n      <td>0.001708</td>\n      <td>0.027211</td>\n      <td>0.009689</td>\n      <td>2295.086182</td>\n      <td>12.0</td>\n    </tr>\n    <tr>\n      <th>53227</th>\n      <td>0.016771</td>\n      <td>0.209713</td>\n      <td>0.010927</td>\n      <td>0.014668</td>\n      <td>0.372532</td>\n      <td>0.070309</td>\n      <td>0.251219</td>\n      <td>0.048426</td>\n      <td>2349.734619</td>\n      <td>0.013585</td>\n      <td>...</td>\n      <td>0.015222</td>\n      <td>0.189199</td>\n      <td>0.003611</td>\n      <td>0.009852</td>\n      <td>0.327217</td>\n      <td>0.077304</td>\n      <td>0.315849</td>\n      <td>0.044695</td>\n      <td>5007.505859</td>\n      <td>11.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>12494 rows × 208 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 228
    }
   ],
   "source": [
    "df_interictal[df.columns[:-2]]"
   ]
  },
  {
   "source": [
    "### train test split"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_interictal_train, X_interictal_test, y_interictal_train, y_interictal_test =train_test_split(X_interictal,y_interictal,test_size=0.1, random_state=42)\n",
    "# X_preictal_train, X_preictal_test, y_preictal_train, y_preictal_test=train_test_split(X_preictal, y_preictal,test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       FP1-F7-0  FP1-F7-1  FP1-F7-2  FP1-F7-3  FP1-F7-4  FP1-F7-5  FP1-F7-6  \\\n",
       "7162   0.024722  0.167940  0.022232  0.040775  0.372914  0.077367  0.253885   \n",
       "7163   0.011673  0.140104  0.005736  0.013079  0.467539  0.068067  0.237187   \n",
       "7164   0.012789  0.161469  0.016808  0.020162  0.463838  0.060573  0.216059   \n",
       "7165   0.026937  0.173863  0.068735  0.064965  0.363154  0.056674  0.210230   \n",
       "7166   0.021215  0.137705  0.071709  0.057182  0.398063  0.055010  0.224288   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "71573  0.057434  0.449707  0.033225  0.051070  0.433269  0.009048  0.022220   \n",
       "71574  0.055667  0.435668  0.037922  0.050661  0.435502  0.009454  0.022743   \n",
       "71575  0.062079  0.444765  0.043678  0.049211  0.426007  0.008424  0.021301   \n",
       "71576  0.053513  0.503538  0.030657  0.035249  0.403018  0.006758  0.017817   \n",
       "71577  0.052255  0.475701  0.021347  0.025140  0.439827  0.008385  0.018634   \n",
       "\n",
       "       FP1-F7-7     FP1-F7-8   F7-T7-0  ...  T8-P8-1-2  T8-P8-1-3  T8-P8-1-4  \\\n",
       "7162   0.053518  1946.139648  0.023040  ...   0.005336   0.014324   0.345691   \n",
       "7163   0.047278  4959.291992  0.029791  ...   0.007012   0.022235   0.356219   \n",
       "7164   0.040374  4813.878418  0.015255  ...   0.004499   0.014204   0.294616   \n",
       "7165   0.039481  2711.432373  0.013499  ...   0.004150   0.010867   0.279594   \n",
       "7166   0.041962  3424.862793  0.015650  ...   0.007664   0.013128   0.336973   \n",
       "...         ...          ...       ...  ...        ...        ...        ...   \n",
       "71573  0.004940   413.782379  0.058471  ...   0.160693   0.272260   0.042583   \n",
       "71574  0.005223   379.738708  0.058253  ...   0.141379   0.150678   0.057322   \n",
       "71575  0.005031   392.124817  0.057531  ...   0.161199   0.189323   0.056296   \n",
       "71576  0.003927   497.231781  0.049436  ...   0.139491   0.192732   0.052969   \n",
       "71577  0.003361   591.865540  0.048061  ...   0.136800   0.186728   0.043668   \n",
       "\n",
       "       T8-P8-1-5  T8-P8-1-6  T8-P8-1-7    T8-P8-1-8  patient  target  \\\n",
       "7162    0.072008   0.255091   0.056759  3028.504395      2.0       1   \n",
       "7163    0.065431   0.226064   0.044866  1992.042847      2.0       1   \n",
       "7164    0.076901   0.262006   0.047901  2765.090088      2.0       1   \n",
       "7165    0.081538   0.272143   0.058216  4110.312988      2.0       1   \n",
       "7166    0.069166   0.258830   0.061734  5443.549805      2.0       1   \n",
       "...          ...        ...        ...          ...      ...     ...   \n",
       "71573   0.000433   0.001810   0.001268   273.556274     12.0       1   \n",
       "71574   0.000523   0.001846   0.001473   226.755234     12.0       1   \n",
       "71575   0.000672   0.003524   0.001930   206.779114     12.0       1   \n",
       "71576   0.000643   0.003340   0.001971   221.903259     12.0       1   \n",
       "71577   0.000433   0.001655   0.001530   255.963150     12.0       1   \n",
       "\n",
       "       seizure_number  \n",
       "7162                3  \n",
       "7163                3  \n",
       "7164                3  \n",
       "7165                3  \n",
       "7166                3  \n",
       "...               ...  \n",
       "71573              39  \n",
       "71574              39  \n",
       "71575              39  \n",
       "71576              39  \n",
       "71577              39  \n",
       "\n",
       "[281 rows x 210 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>FP1-F7-0</th>\n      <th>FP1-F7-1</th>\n      <th>FP1-F7-2</th>\n      <th>FP1-F7-3</th>\n      <th>FP1-F7-4</th>\n      <th>FP1-F7-5</th>\n      <th>FP1-F7-6</th>\n      <th>FP1-F7-7</th>\n      <th>FP1-F7-8</th>\n      <th>F7-T7-0</th>\n      <th>...</th>\n      <th>T8-P8-1-2</th>\n      <th>T8-P8-1-3</th>\n      <th>T8-P8-1-4</th>\n      <th>T8-P8-1-5</th>\n      <th>T8-P8-1-6</th>\n      <th>T8-P8-1-7</th>\n      <th>T8-P8-1-8</th>\n      <th>patient</th>\n      <th>target</th>\n      <th>seizure_number</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>7162</th>\n      <td>0.024722</td>\n      <td>0.167940</td>\n      <td>0.022232</td>\n      <td>0.040775</td>\n      <td>0.372914</td>\n      <td>0.077367</td>\n      <td>0.253885</td>\n      <td>0.053518</td>\n      <td>1946.139648</td>\n      <td>0.023040</td>\n      <td>...</td>\n      <td>0.005336</td>\n      <td>0.014324</td>\n      <td>0.345691</td>\n      <td>0.072008</td>\n      <td>0.255091</td>\n      <td>0.056759</td>\n      <td>3028.504395</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>7163</th>\n      <td>0.011673</td>\n      <td>0.140104</td>\n      <td>0.005736</td>\n      <td>0.013079</td>\n      <td>0.467539</td>\n      <td>0.068067</td>\n      <td>0.237187</td>\n      <td>0.047278</td>\n      <td>4959.291992</td>\n      <td>0.029791</td>\n      <td>...</td>\n      <td>0.007012</td>\n      <td>0.022235</td>\n      <td>0.356219</td>\n      <td>0.065431</td>\n      <td>0.226064</td>\n      <td>0.044866</td>\n      <td>1992.042847</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>7164</th>\n      <td>0.012789</td>\n      <td>0.161469</td>\n      <td>0.016808</td>\n      <td>0.020162</td>\n      <td>0.463838</td>\n      <td>0.060573</td>\n      <td>0.216059</td>\n      <td>0.040374</td>\n      <td>4813.878418</td>\n      <td>0.015255</td>\n      <td>...</td>\n      <td>0.004499</td>\n      <td>0.014204</td>\n      <td>0.294616</td>\n      <td>0.076901</td>\n      <td>0.262006</td>\n      <td>0.047901</td>\n      <td>2765.090088</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>7165</th>\n      <td>0.026937</td>\n      <td>0.173863</td>\n      <td>0.068735</td>\n      <td>0.064965</td>\n      <td>0.363154</td>\n      <td>0.056674</td>\n      <td>0.210230</td>\n      <td>0.039481</td>\n      <td>2711.432373</td>\n      <td>0.013499</td>\n      <td>...</td>\n      <td>0.004150</td>\n      <td>0.010867</td>\n      <td>0.279594</td>\n      <td>0.081538</td>\n      <td>0.272143</td>\n      <td>0.058216</td>\n      <td>4110.312988</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>7166</th>\n      <td>0.021215</td>\n      <td>0.137705</td>\n      <td>0.071709</td>\n      <td>0.057182</td>\n      <td>0.398063</td>\n      <td>0.055010</td>\n      <td>0.224288</td>\n      <td>0.041962</td>\n      <td>3424.862793</td>\n      <td>0.015650</td>\n      <td>...</td>\n      <td>0.007664</td>\n      <td>0.013128</td>\n      <td>0.336973</td>\n      <td>0.069166</td>\n      <td>0.258830</td>\n      <td>0.061734</td>\n      <td>5443.549805</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>71573</th>\n      <td>0.057434</td>\n      <td>0.449707</td>\n      <td>0.033225</td>\n      <td>0.051070</td>\n      <td>0.433269</td>\n      <td>0.009048</td>\n      <td>0.022220</td>\n      <td>0.004940</td>\n      <td>413.782379</td>\n      <td>0.058471</td>\n      <td>...</td>\n      <td>0.160693</td>\n      <td>0.272260</td>\n      <td>0.042583</td>\n      <td>0.000433</td>\n      <td>0.001810</td>\n      <td>0.001268</td>\n      <td>273.556274</td>\n      <td>12.0</td>\n      <td>1</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>71574</th>\n      <td>0.055667</td>\n      <td>0.435668</td>\n      <td>0.037922</td>\n      <td>0.050661</td>\n      <td>0.435502</td>\n      <td>0.009454</td>\n      <td>0.022743</td>\n      <td>0.005223</td>\n      <td>379.738708</td>\n      <td>0.058253</td>\n      <td>...</td>\n      <td>0.141379</td>\n      <td>0.150678</td>\n      <td>0.057322</td>\n      <td>0.000523</td>\n      <td>0.001846</td>\n      <td>0.001473</td>\n      <td>226.755234</td>\n      <td>12.0</td>\n      <td>1</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>71575</th>\n      <td>0.062079</td>\n      <td>0.444765</td>\n      <td>0.043678</td>\n      <td>0.049211</td>\n      <td>0.426007</td>\n      <td>0.008424</td>\n      <td>0.021301</td>\n      <td>0.005031</td>\n      <td>392.124817</td>\n      <td>0.057531</td>\n      <td>...</td>\n      <td>0.161199</td>\n      <td>0.189323</td>\n      <td>0.056296</td>\n      <td>0.000672</td>\n      <td>0.003524</td>\n      <td>0.001930</td>\n      <td>206.779114</td>\n      <td>12.0</td>\n      <td>1</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>71576</th>\n      <td>0.053513</td>\n      <td>0.503538</td>\n      <td>0.030657</td>\n      <td>0.035249</td>\n      <td>0.403018</td>\n      <td>0.006758</td>\n      <td>0.017817</td>\n      <td>0.003927</td>\n      <td>497.231781</td>\n      <td>0.049436</td>\n      <td>...</td>\n      <td>0.139491</td>\n      <td>0.192732</td>\n      <td>0.052969</td>\n      <td>0.000643</td>\n      <td>0.003340</td>\n      <td>0.001971</td>\n      <td>221.903259</td>\n      <td>12.0</td>\n      <td>1</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>71577</th>\n      <td>0.052255</td>\n      <td>0.475701</td>\n      <td>0.021347</td>\n      <td>0.025140</td>\n      <td>0.439827</td>\n      <td>0.008385</td>\n      <td>0.018634</td>\n      <td>0.003361</td>\n      <td>591.865540</td>\n      <td>0.048061</td>\n      <td>...</td>\n      <td>0.136800</td>\n      <td>0.186728</td>\n      <td>0.043668</td>\n      <td>0.000433</td>\n      <td>0.001655</td>\n      <td>0.001530</td>\n      <td>255.963150</td>\n      <td>12.0</td>\n      <td>1</td>\n      <td>39</td>\n    </tr>\n  </tbody>\n</table>\n<p>281 rows × 210 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 231
    }
   ],
   "source": [
    "interictal_test_df=pd.DataFrame(data=X_interictal_test, columns=generate_column_names())\n",
    "interictal_test_df['target']=y_interictal_test\n",
    "interictal_test_df\n",
    "# test_df=test_df.append(interictal_test_df, ignore_index=True)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = np.concatenate((X_interictal_train, X_preictal_train))\n",
    "# y_train = np.concatenate((y_interictal_train, y_preictal_train))\n",
    "# X_test = np.array(test_df[df.columns[:-2]]).astype('float32')\n",
    "# y_test  =np.array(test_df['target']).astype('float32')\n",
    "X_train = np.concatenate((X_interictal_train, X_preictal_train))\n",
    "y_train = np.concatenate((y_interictal_train, y_preictal_train))\n",
    "X_test = np.array(test_df[df.columns[:-2]]).astype('float32')\n",
    "y_test  =np.array(test_df['target']).astype('float32')\n",
    "X_test = np.concatenate((X_test, X_interictal_test))\n",
    "y_test  =np.concatenate((y_test, y_interictal_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(12763, 208)\n(1531, 208)\n(12763,)\n(1531,)\n"
     ]
    }
   ],
   "source": [
    "X_train_shape =X_train.shape\n",
    "X_test_shape = X_test.shape\n",
    "y_train_shape =y_train.shape \n",
    "y_test_shape = y_test.shape\n",
    "print(X_train_shape)\n",
    "print(X_test_shape)\n",
    "print(y_train_shape)\n",
    "print(y_test_shape)"
   ]
  },
  {
   "source": [
    "### Normalization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "X_train=scaler.transform(X_train)\n",
    "X_test=scaler.transform(X_test)"
   ]
  },
  {
   "source": [
    "### encoding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1531, 2)\n1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "# encoded_Y = encoder.fit_transform(y)\n",
    "enc_y_train = np_utils.to_categorical(y_train)\n",
    "enc_y_test = np_utils.to_categorical(y_test)\n",
    "print(enc_y_test.shape)\n",
    "print(enc_y_test.max())"
   ]
  },
  {
   "source": [
    "### training the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_9\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlayer1 (Dense)               (None, 256)               53504     \n_________________________________________________________________\nhidden1 (Dense)              (None, 256)               65792     \n_________________________________________________________________\nhidden2 (Dense)              (None, 128)               32896     \n_________________________________________________________________\nhidden3 (Dense)              (None, 128)               16512     \n_________________________________________________________________\nhidden4 (Dense)              (None, 64)                8256      \n_________________________________________________________________\nhidden5 (Dense)              (None, 32)                2080      \n_________________________________________________________________\noutput (Dense)               (None, 2)                 66        \n=================================================================\nTotal params: 179,106\nTrainable params: 179,106\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        Dense(256, name=\"layer1\", input_shape=(208,), activation = 'linear' ),\n",
    "        Dense(256, name=\"hidden1\", activation = 'linear'),\n",
    "        Dense(128, name=\"hidden2\", activation = 'linear'),\n",
    "        Dense(128, name=\"hidden3\", activation = 'linear'),\n",
    "        Dense(64, name=\"hidden4\", activation = 'linear'),\n",
    "        Dense(32, name=\"hidden5\", activation = 'linear'),\n",
    "        Dense(2, name=\"output\", activation = 'sigmoid'),\n",
    "    ]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=keras.optimizers.Adam(learning_rate=0.05)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy', tf.keras.metrics.FalseNegatives(thresholds=None, name=None, dtype=None)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "racy: 0.8672 - false_negatives_9: 875.7650 - val_loss: 0.4991 - val_accuracy: 0.7747 - val_false_negatives_9: 340.0000\n",
      "Epoch 106/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0527 - accuracy: 0.8707 - false_negatives_9: 847.3825 - val_loss: 0.5008 - val_accuracy: 0.7662 - val_false_negatives_9: 346.0000\n",
      "Epoch 107/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0545 - accuracy: 0.8594 - false_negatives_9: 896.7625 - val_loss: 0.5016 - val_accuracy: 0.7890 - val_false_negatives_9: 322.0000\n",
      "Epoch 108/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0519 - accuracy: 0.8769 - false_negatives_9: 829.8650 - val_loss: 0.4803 - val_accuracy: 0.8021 - val_false_negatives_9: 302.0000\n",
      "Epoch 109/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0586 - accuracy: 0.8556 - false_negatives_9: 910.4400 - val_loss: 0.5885 - val_accuracy: 0.7675 - val_false_negatives_9: 355.0000\n",
      "Epoch 110/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0556 - accuracy: 0.8594 - false_negatives_9: 905.9800 - val_loss: 0.4946 - val_accuracy: 0.7688 - val_false_negatives_9: 357.0000\n",
      "Epoch 111/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0558 - accuracy: 0.8622 - false_negatives_9: 882.0625 - val_loss: 0.4776 - val_accuracy: 0.8001 - val_false_negatives_9: 307.0000\n",
      "Epoch 112/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0562 - accuracy: 0.8511 - false_negatives_9: 930.5275 - val_loss: 0.5146 - val_accuracy: 0.7831 - val_false_negatives_9: 332.0000\n",
      "Epoch 113/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0545 - accuracy: 0.8656 - false_negatives_9: 863.6375 - val_loss: 0.4970 - val_accuracy: 0.7740 - val_false_negatives_9: 346.0000\n",
      "Epoch 114/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0536 - accuracy: 0.8647 - false_negatives_9: 861.3225 - val_loss: 0.5172 - val_accuracy: 0.7792 - val_false_negatives_9: 338.0000\n",
      "Epoch 115/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0536 - accuracy: 0.8619 - false_negatives_9: 883.6425 - val_loss: 0.5166 - val_accuracy: 0.7831 - val_false_negatives_9: 332.0000\n",
      "Epoch 116/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0551 - accuracy: 0.8613 - false_negatives_9: 870.8450 - val_loss: 0.5160 - val_accuracy: 0.7675 - val_false_negatives_9: 357.0000\n",
      "Epoch 117/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0548 - accuracy: 0.8614 - false_negatives_9: 885.1850 - val_loss: 0.5324 - val_accuracy: 0.7753 - val_false_negatives_9: 344.0000\n",
      "Epoch 118/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0551 - accuracy: 0.8628 - false_negatives_9: 877.9875 - val_loss: 0.4872 - val_accuracy: 0.7779 - val_false_negatives_9: 340.0000\n",
      "Epoch 119/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0542 - accuracy: 0.8619 - false_negatives_9: 876.2450 - val_loss: 0.4422 - val_accuracy: 0.7943 - val_false_negatives_9: 315.0000\n",
      "Epoch 120/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0540 - accuracy: 0.8698 - false_negatives_9: 857.4100 - val_loss: 0.5089 - val_accuracy: 0.7766 - val_false_negatives_9: 342.0000\n",
      "Epoch 121/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0541 - accuracy: 0.8629 - false_negatives_9: 881.1075 - val_loss: 0.4978 - val_accuracy: 0.7838 - val_false_negatives_9: 330.0000\n",
      "Epoch 122/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0554 - accuracy: 0.8702 - false_negatives_9: 839.5175 - val_loss: 0.5646 - val_accuracy: 0.7609 - val_false_negatives_9: 366.0000\n",
      "Epoch 123/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0557 - accuracy: 0.8609 - false_negatives_9: 886.9500 - val_loss: 0.6456 - val_accuracy: 0.7629 - val_false_negatives_9: 354.0000\n",
      "Epoch 124/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0574 - accuracy: 0.8565 - false_negatives_9: 908.1250 - val_loss: 0.4910 - val_accuracy: 0.7890 - val_false_negatives_9: 323.0000\n",
      "Epoch 125/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0544 - accuracy: 0.8676 - false_negatives_9: 879.8300 - val_loss: 0.5366 - val_accuracy: 0.7570 - val_false_negatives_9: 371.0000\n",
      "Epoch 126/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0518 - accuracy: 0.8670 - false_negatives_9: 844.4750 - val_loss: 0.4684 - val_accuracy: 0.7766 - val_false_negatives_9: 342.0000\n",
      "Epoch 127/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0551 - accuracy: 0.8621 - false_negatives_9: 882.2950 - val_loss: 0.4777 - val_accuracy: 0.7969 - val_false_negatives_9: 311.0000\n",
      "Epoch 128/200\n",
      "399/399 [==============================] - 1s 3ms/step - loss: 0.0526 - accuracy: 0.8755 - false_negatives_9: 838.4850 - val_loss: 0.5895 - val_accuracy: 0.7466 - val_false_negatives_9: 388.0000\n",
      "Epoch 129/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0519 - accuracy: 0.8741 - false_negatives_9: 826.6925 - val_loss: 0.4928 - val_accuracy: 0.7688 - val_false_negatives_9: 350.0000\n",
      "Epoch 130/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0549 - accuracy: 0.8607 - false_negatives_9: 886.4550 - val_loss: 0.5645 - val_accuracy: 0.7583 - val_false_negatives_9: 371.0000\n",
      "Epoch 131/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0553 - accuracy: 0.8590 - false_negatives_9: 880.8600 - val_loss: 0.4977 - val_accuracy: 0.8054 - val_false_negatives_9: 298.0000\n",
      "Epoch 132/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0529 - accuracy: 0.8706 - false_negatives_9: 850.0400 - val_loss: 0.4966 - val_accuracy: 0.7773 - val_false_negatives_9: 341.0000\n",
      "Epoch 133/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0524 - accuracy: 0.8710 - false_negatives_9: 828.5025 - val_loss: 0.5170 - val_accuracy: 0.7831 - val_false_negatives_9: 334.0000\n",
      "Epoch 134/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0534 - accuracy: 0.8672 - false_negatives_9: 866.3100 - val_loss: 0.4859 - val_accuracy: 0.8040 - val_false_negatives_9: 299.0000\n",
      "Epoch 135/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0564 - accuracy: 0.8606 - false_negatives_9: 887.3775 - val_loss: 0.5001 - val_accuracy: 0.7858 - val_false_negatives_9: 328.0000\n",
      "Epoch 136/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0536 - accuracy: 0.8678 - false_negatives_9: 860.0700 - val_loss: 0.4912 - val_accuracy: 0.7799 - val_false_negatives_9: 337.0000\n",
      "Epoch 137/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0536 - accuracy: 0.8716 - false_negatives_9: 839.4550 - val_loss: 0.5169 - val_accuracy: 0.7786 - val_false_negatives_9: 339.0000\n",
      "Epoch 138/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0554 - accuracy: 0.8626 - false_negatives_9: 876.4300 - val_loss: 0.6397 - val_accuracy: 0.7740 - val_false_negatives_9: 347.0000\n",
      "Epoch 139/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0556 - accuracy: 0.8633 - false_negatives_9: 879.2975 - val_loss: 0.4264 - val_accuracy: 0.8106 - val_false_negatives_9: 287.0000\n",
      "Epoch 140/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0542 - accuracy: 0.8687 - false_negatives_9: 853.1375 - val_loss: 0.5334 - val_accuracy: 0.7707 - val_false_negatives_9: 350.0000\n",
      "Epoch 141/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0550 - accuracy: 0.8620 - false_negatives_9: 884.7975 - val_loss: 0.4939 - val_accuracy: 0.7786 - val_false_negatives_9: 339.0000\n",
      "Epoch 142/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0541 - accuracy: 0.8676 - false_negatives_9: 861.0250 - val_loss: 0.4830 - val_accuracy: 0.7831 - val_false_negatives_9: 332.0000\n",
      "Epoch 143/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0545 - accuracy: 0.8666 - false_negatives_9: 845.2075 - val_loss: 0.5543 - val_accuracy: 0.7609 - val_false_negatives_9: 366.0000\n",
      "Epoch 144/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0515 - accuracy: 0.8707 - false_negatives_9: 838.3225 - val_loss: 0.4945 - val_accuracy: 0.7714 - val_false_negatives_9: 350.0000\n",
      "Epoch 145/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0548 - accuracy: 0.8625 - false_negatives_9: 894.2950 - val_loss: 0.5553 - val_accuracy: 0.7629 - val_false_negatives_9: 356.0000\n",
      "Epoch 146/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0568 - accuracy: 0.8507 - false_negatives_9: 934.2375 - val_loss: 0.4746 - val_accuracy: 0.7714 - val_false_negatives_9: 350.0000\n",
      "Epoch 147/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0536 - accuracy: 0.8678 - false_negatives_9: 857.4650 - val_loss: 0.4759 - val_accuracy: 0.7890 - val_false_negatives_9: 322.0000\n",
      "Epoch 148/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0573 - accuracy: 0.8639 - false_negatives_9: 863.9725 - val_loss: 0.4861 - val_accuracy: 0.7838 - val_false_negatives_9: 331.0000\n",
      "Epoch 149/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0562 - accuracy: 0.8578 - false_negatives_9: 900.4075 - val_loss: 0.5038 - val_accuracy: 0.7877 - val_false_negatives_9: 326.0000\n",
      "Epoch 150/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0551 - accuracy: 0.8641 - false_negatives_9: 886.4525 - val_loss: 0.4911 - val_accuracy: 0.8119 - val_false_negatives_9: 288.0000\n",
      "Epoch 151/200\n",
      "399/399 [==============================] - 1s 3ms/step - loss: 0.0531 - accuracy: 0.8702 - false_negatives_9: 840.7925 - val_loss: 0.4898 - val_accuracy: 0.7858 - val_false_negatives_9: 328.0000\n",
      "Epoch 152/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0545 - accuracy: 0.8600 - false_negatives_9: 887.4975 - val_loss: 0.5669 - val_accuracy: 0.7799 - val_false_negatives_9: 337.0000\n",
      "Epoch 153/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0546 - accuracy: 0.8637 - false_negatives_9: 865.4950 - val_loss: 0.4625 - val_accuracy: 0.7962 - val_false_negatives_9: 312.0000\n",
      "Epoch 154/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0528 - accuracy: 0.8747 - false_negatives_9: 825.2625 - val_loss: 0.4856 - val_accuracy: 0.7943 - val_false_negatives_9: 317.0000\n",
      "Epoch 155/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0554 - accuracy: 0.8661 - false_negatives_9: 873.7575 - val_loss: 0.5995 - val_accuracy: 0.7446 - val_false_negatives_9: 389.0000\n",
      "Epoch 156/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0563 - accuracy: 0.8606 - false_negatives_9: 870.0800 - val_loss: 0.4978 - val_accuracy: 0.7740 - val_false_negatives_9: 346.0000\n",
      "Epoch 157/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0518 - accuracy: 0.8747 - false_negatives_9: 819.9300 - val_loss: 0.4470 - val_accuracy: 0.8165 - val_false_negatives_9: 281.0000\n",
      "Epoch 158/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0540 - accuracy: 0.8688 - false_negatives_9: 845.1275 - val_loss: 0.5409 - val_accuracy: 0.7747 - val_false_negatives_9: 345.0000\n",
      "Epoch 159/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0559 - accuracy: 0.8526 - false_negatives_9: 904.2200 - val_loss: 0.4801 - val_accuracy: 0.7929 - val_false_negatives_9: 317.0000\n",
      "Epoch 160/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0529 - accuracy: 0.8668 - false_negatives_9: 862.1675 - val_loss: 0.5081 - val_accuracy: 0.7792 - val_false_negatives_9: 338.0000\n",
      "Epoch 161/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0528 - accuracy: 0.8734 - false_negatives_9: 826.9225 - val_loss: 0.5982 - val_accuracy: 0.7368 - val_false_negatives_9: 403.0000\n",
      "Epoch 162/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0566 - accuracy: 0.8554 - false_negatives_9: 890.4650 - val_loss: 0.5581 - val_accuracy: 0.7622 - val_false_negatives_9: 364.0000\n",
      "Epoch 163/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0533 - accuracy: 0.8680 - false_negatives_9: 855.9075 - val_loss: 0.5091 - val_accuracy: 0.7851 - val_false_negatives_9: 329.0000\n",
      "Epoch 164/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0545 - accuracy: 0.8675 - false_negatives_9: 854.8825 - val_loss: 0.4441 - val_accuracy: 0.8099 - val_false_negatives_9: 291.0000\n",
      "Epoch 165/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0529 - accuracy: 0.8722 - false_negatives_9: 831.2850 - val_loss: 0.4760 - val_accuracy: 0.7903 - val_false_negatives_9: 321.0000\n",
      "Epoch 166/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0551 - accuracy: 0.8600 - false_negatives_9: 889.7175 - val_loss: 0.4778 - val_accuracy: 0.7799 - val_false_negatives_9: 335.0000\n",
      "Epoch 167/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0541 - accuracy: 0.8690 - false_negatives_9: 850.5250 - val_loss: 0.4999 - val_accuracy: 0.7818 - val_false_negatives_9: 335.0000\n",
      "Epoch 168/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0551 - accuracy: 0.8639 - false_negatives_9: 874.3475 - val_loss: 0.5282 - val_accuracy: 0.7838 - val_false_negatives_9: 332.0000\n",
      "Epoch 169/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0529 - accuracy: 0.8670 - false_negatives_9: 848.4625 - val_loss: 0.4840 - val_accuracy: 0.7877 - val_false_negatives_9: 325.0000\n",
      "Epoch 170/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0533 - accuracy: 0.8749 - false_negatives_9: 829.2750 - val_loss: 0.5383 - val_accuracy: 0.8040 - val_false_negatives_9: 300.0000\n",
      "Epoch 171/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0537 - accuracy: 0.8691 - false_negatives_9: 852.9650 - val_loss: 0.4479 - val_accuracy: 0.8138 - val_false_negatives_9: 285.0000\n",
      "Epoch 172/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0536 - accuracy: 0.8748 - false_negatives_9: 826.4600 - val_loss: 0.5294 - val_accuracy: 0.7753 - val_false_negatives_9: 344.0000\n",
      "Epoch 173/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0557 - accuracy: 0.8567 - false_negatives_9: 894.4475 - val_loss: 0.5317 - val_accuracy: 0.7805 - val_false_negatives_9: 336.0000\n",
      "Epoch 174/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0551 - accuracy: 0.8582 - false_negatives_9: 894.4400 - val_loss: 0.5331 - val_accuracy: 0.7590 - val_false_negatives_9: 369.0000\n",
      "Epoch 175/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0556 - accuracy: 0.8624 - false_negatives_9: 859.2075 - val_loss: 0.5101 - val_accuracy: 0.7864 - val_false_negatives_9: 327.0000\n",
      "Epoch 176/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0522 - accuracy: 0.8704 - false_negatives_9: 845.9500 - val_loss: 0.5285 - val_accuracy: 0.7995 - val_false_negatives_9: 306.0000\n",
      "Epoch 177/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0568 - accuracy: 0.8634 - false_negatives_9: 866.8550 - val_loss: 0.5024 - val_accuracy: 0.7916 - val_false_negatives_9: 319.0000\n",
      "Epoch 178/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0544 - accuracy: 0.8677 - false_negatives_9: 846.9000 - val_loss: 0.4760 - val_accuracy: 0.7812 - val_false_negatives_9: 335.0000\n",
      "Epoch 179/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0532 - accuracy: 0.8652 - false_negatives_9: 856.5150 - val_loss: 0.5042 - val_accuracy: 0.7688 - val_false_negatives_9: 354.0000\n",
      "Epoch 180/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0558 - accuracy: 0.8603 - false_negatives_9: 861.8325 - val_loss: 0.4982 - val_accuracy: 0.7818 - val_false_negatives_9: 334.0000\n",
      "Epoch 181/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0521 - accuracy: 0.8699 - false_negatives_9: 837.1275 - val_loss: 0.5148 - val_accuracy: 0.7766 - val_false_negatives_9: 342.0000\n",
      "Epoch 182/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0569 - accuracy: 0.8591 - false_negatives_9: 887.1800 - val_loss: 0.5714 - val_accuracy: 0.7681 - val_false_negatives_9: 355.0000\n",
      "Epoch 183/200\n",
      "399/399 [==============================] - 1s 3ms/step - loss: 0.0548 - accuracy: 0.8640 - false_negatives_9: 855.0750 - val_loss: 0.4727 - val_accuracy: 0.7916 - val_false_negatives_9: 319.0000\n",
      "Epoch 184/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0516 - accuracy: 0.8787 - false_negatives_9: 816.7800 - val_loss: 0.4955 - val_accuracy: 0.7903 - val_false_negatives_9: 321.0000\n",
      "Epoch 185/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0525 - accuracy: 0.8712 - false_negatives_9: 830.6250 - val_loss: 0.4998 - val_accuracy: 0.7642 - val_false_negatives_9: 361.0000\n",
      "Epoch 186/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0523 - accuracy: 0.8709 - false_negatives_9: 835.3325 - val_loss: 0.5810 - val_accuracy: 0.7544 - val_false_negatives_9: 376.0000\n",
      "Epoch 187/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0529 - accuracy: 0.8703 - false_negatives_9: 839.9525 - val_loss: 0.4475 - val_accuracy: 0.7871 - val_false_negatives_9: 331.0000\n",
      "Epoch 188/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0530 - accuracy: 0.8705 - false_negatives_9: 852.4050 - val_loss: 0.4900 - val_accuracy: 0.7995 - val_false_negatives_9: 307.0000\n",
      "Epoch 189/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0549 - accuracy: 0.8629 - false_negatives_9: 877.6650 - val_loss: 0.4865 - val_accuracy: 0.7988 - val_false_negatives_9: 308.0000\n",
      "Epoch 190/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0542 - accuracy: 0.8645 - false_negatives_9: 886.3275 - val_loss: 0.5301 - val_accuracy: 0.7858 - val_false_negatives_9: 328.0000\n",
      "Epoch 191/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0517 - accuracy: 0.8767 - false_negatives_9: 823.9300 - val_loss: 0.5028 - val_accuracy: 0.7773 - val_false_negatives_9: 341.0000\n",
      "Epoch 192/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0524 - accuracy: 0.8703 - false_negatives_9: 855.5550 - val_loss: 0.5581 - val_accuracy: 0.7858 - val_false_negatives_9: 325.0000\n",
      "Epoch 193/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0537 - accuracy: 0.8696 - false_negatives_9: 851.2650 - val_loss: 0.5199 - val_accuracy: 0.7779 - val_false_negatives_9: 340.0000\n",
      "Epoch 194/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0554 - accuracy: 0.8606 - false_negatives_9: 875.6625 - val_loss: 0.5769 - val_accuracy: 0.7642 - val_false_negatives_9: 361.0000\n",
      "Epoch 195/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0515 - accuracy: 0.8719 - false_negatives_9: 845.2425 - val_loss: 0.5814 - val_accuracy: 0.7616 - val_false_negatives_9: 365.0000\n",
      "Epoch 196/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0528 - accuracy: 0.8716 - false_negatives_9: 845.3925 - val_loss: 0.5018 - val_accuracy: 0.8040 - val_false_negatives_9: 300.0000\n",
      "Epoch 197/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0540 - accuracy: 0.8682 - false_negatives_9: 858.7350 - val_loss: 0.5269 - val_accuracy: 0.7629 - val_false_negatives_9: 363.0000\n",
      "Epoch 198/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0522 - accuracy: 0.8696 - false_negatives_9: 838.2050 - val_loss: 0.4764 - val_accuracy: 0.8054 - val_false_negatives_9: 298.0000\n",
      "Epoch 199/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0526 - accuracy: 0.8708 - false_negatives_9: 844.4850 - val_loss: 0.4908 - val_accuracy: 0.7838 - val_false_negatives_9: 330.0000\n",
      "Epoch 200/200\n",
      "399/399 [==============================] - 1s 2ms/step - loss: 0.0528 - accuracy: 0.8756 - false_negatives_9: 812.6625 - val_loss: 0.5150 - val_accuracy: 0.7609 - val_false_negatives_9: 366.0000\n",
      "22:43:57\n",
      "saving new model\n"
     ]
    }
   ],
   "source": [
    "before = datetime.now()\n",
    "before_time =before.strftime(\"%H:%M:%S\")\n",
    "print(before_time)\n",
    "class_weight = {0: 0.1, 1: 0.9}\n",
    "model.fit(X_train, enc_y_train, epochs=200, batch_size= 32, validation_data=(X_test, enc_y_test), class_weight=class_weight)\n",
    "\n",
    "after = datetime.now()\n",
    "after_time =after.strftime(\"%H:%M:%S\")\n",
    "print(after_time)\n",
    "\n",
    "print (\"saving new model\")\n",
    "model.save(\"DNNseizure.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "48/48 [==============================] - 0s 1ms/step - loss: 0.5150 - accuracy: 0.7609 - false_negatives_9: 366.0000\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.5149813890457153, 0.7609405517578125, 366.0]"
      ]
     },
     "metadata": {},
     "execution_count": 240
    }
   ],
   "source": [
    "model.evaluate(X_test, enc_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[9.9389040e-01, 6.1482787e-03],\n",
       "       [9.7671568e-01, 2.3385584e-02],\n",
       "       [9.6795124e-01, 3.2190770e-02],\n",
       "       ...,\n",
       "       [7.3068601e-01, 2.6935992e-01],\n",
       "       [9.9997461e-01, 2.5775556e-05],\n",
       "       [9.6060961e-01, 3.9581358e-02]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 241
    }
   ],
   "source": [
    "# loaded_clf = joblib.load(\"my_models/SVM_blanced_chb04.pkl\")\n",
    "# y_pred=loaded_clf.predict(X_test)\n",
    "y_pred1=model.predict(X_test)\n",
    "y_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1531,)"
      ]
     },
     "metadata": {},
     "execution_count": 242
    }
   ],
   "source": [
    "y_pred=[0]*len(y_pred1)\n",
    "i=0\n",
    "for pr in y_pred1:\n",
    "    y_pred[i]=np.argmax(pr)\n",
    "    i+=1\n",
    "y_pred=np.array(y_pred)\n",
    "y_pred.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_clf.get_params(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TN:1015, FP:235, FN:131, TP:150\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = sklearn.metrics.confusion_matrix(y_test, y_pred).ravel()\n",
    "print(\"TN:{}, FP:{}, FN:{}, TP:{}\".format(tn, fp, fn, tp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "specificity= 0.812 , sensitivity= 0.5338078291814946\n"
     ]
    }
   ],
   "source": [
    "specificity=(tn)/(tn+fp)\n",
    "sensitivity=(tp)/(tp+fn)\n",
    "print('specificity= {} , sensitivity= {}'.format(specificity, sensitivity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X size =(71760, 208), y size = (71760,)\ninterictal size =(12494, 210), preictal size = (1519, 210)\ntrain size =(12763, 208), test size = (1531, 208)\nTN:1015, FP:235, FN:131, TP:150\nspecificity= 0.812 , sensitivity= 0.5338078291814946\n"
     ]
    }
   ],
   "source": [
    "print('X size ={}, y size = {}'.format(X_shape, y_shape))\n",
    "print('interictal size ={}, preictal size = {}'.format(interictal_shape, preictal_shape))\n",
    "print('train size ={}, test size = {}'.format(X_train_shape, X_test_shape))\n",
    "\n",
    "# print(\"Training Accuracy: %d\"%(trainAcc*100)+\"%\")\n",
    "# print(\"Testing Accuracy: %d\"%(testAcc *100)+\"%\")\n",
    "print(\"TN:{}, FP:{}, FN:{}, TP:{}\".format(tn, fp, fn, tp))\n",
    "print('specificity= {} , sensitivity= {}'.format(specificity, sensitivity))"
   ]
  },
  {
   "source": [
    "### grid search"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numpy import mean\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.model_selection import RepeatedStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance = [{0:10,2:1}, {0:1,2:1}, {0:1,2:10}, {0:1,2:50}, {0:1,2:100}]\n",
    "# costs=[1.0,10.0,100.0]\n",
    "# param_grid = dict(C=costs, class_weight=balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
    "# grid = GridSearchCV(estimator=clf, param_grid=param_grid, n_jobs=-1, cv=cv, scoring='f1_weighted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "# means = grid_result.cv_results_['mean_test_score']\n",
    "# stds = grid_result.cv_results_['std_test_score']\n",
    "# params = grid_result.cv_results_['params']\n",
    "# for mean, stdev, param in zip(means, stds, params):\n",
    "#     print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}